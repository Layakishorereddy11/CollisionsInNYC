[
  {
    "objectID": "03_raw_to_tabular.html",
    "href": "03_raw_to_tabular.html",
    "title": "3: From raw to tabular data",
    "section": "",
    "text": "Here I have extracted 10000000 rows from api, and stored in the pandas dataframe and displayed latest 10 rows below\n[‘crash_date’, ‘crash_time’, ‘on_street_name’, ‘off_street_name’, ‘number_of_persons_injured’, ‘number_of_persons_killed’, ‘number_of_pedestrians_injured’, ‘number_of_pedestrians_killed’, ‘number_of_cyclist_injured’, ‘number_of_cyclist_killed’, ‘number_of_motorist_injured’, ‘number_of_motorist_killed’, ‘contributing_factor_vehicle_1’, ‘contributing_factor_vehicle_2’, ‘collision_id’, ‘vehicle_type_code1’, ‘vehicle_type_code2’, ‘borough’, ‘zip_code’, ‘latitude’, ‘longitude’, ‘location’, ‘cross_street_name’, ‘contributing_factor_vehicle_3’, ‘vehicle_type_code_3’, ‘contributing_factor_vehicle_4’, ‘vehicle_type_code_4’, ‘contributing_factor_vehicle_5’, ‘vehicle_type_code_5’]\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport pydeck as pdk\nimport plotly.express as px\nfrom sodapy import Socrata\nimport pandas as pd\n# import pandahelper.reports as ph\nfrom datetime import datetime\nimport urllib\n\ndef dataloading():\n    query = \"\"\"\n        SELECT\n            *\n        WHERE\n            crash_date &gt;= '2013'\n        ORDER BY\n            crash_date DESC\n        LIMIT\n            1000000000\n        \"\"\"\n\n    # format values in the url, easier to read\n    safe_string = urllib.parse.quote_plus(query)\n    print(safe_string)\n\n    # compose url\n    url = 'https://data.cityofnewyork.us/resource/h9gi-nx95.json?$query={}'.format(safe_string)\n    print('url:', url)\n    return url\n\n\ndf = pd.read_json(dataloading())\n\nprint(df.shape)\ndf.head()\n\n%0A++++++++SELECT%0A++++++++++++%2A%0A++++++++WHERE%0A++++++++++++crash_date+%3E%3D+%272013%27%0A++++++++ORDER+BY%0A++++++++++++crash_date+DESC%0A++++++++LIMIT%0A++++++++++++1000000000%0A++++++++\nurl: https://data.cityofnewyork.us/resource/h9gi-nx95.json?$query=%0A++++++++SELECT%0A++++++++++++%2A%0A++++++++WHERE%0A++++++++++++crash_date+%3E%3D+%272013%27%0A++++++++ORDER+BY%0A++++++++++++crash_date+DESC%0A++++++++LIMIT%0A++++++++++++1000000000%0A++++++++\n(2030615, 29)\n\n\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\ncross_street_name\nnumber_of_persons_injured\nnumber_of_persons_killed\n...\nvehicle_type_code1\non_street_name\noff_street_name\nvehicle_type_code2\ncontributing_factor_vehicle_3\nvehicle_type_code_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n2024-10-29T00:00:00.000\n2024-11-03 09:40:00\nBRONX\n10460\n40.84033\n-73.876945\n{'latitude': '40.84033', 'longitude': '-73.876...\n1101 E TREMONT AVE\n0.0\n0.0\n...\nSedan\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2024-10-29T00:00:00.000\n2024-11-03 01:00:00\nBRONX\n10461\n40.84868\n-73.852590\n{'latitude': '40.84868', 'longitude': '-73.852...\n1721 HAIGHT AVE\n0.0\n0.0\n...\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2024-10-29T00:00:00.000\n2024-11-03 18:55:00\nBROOKLYN\n11235\n40.59287\n-73.949110\n{'latitude': '40.59287', 'longitude': '-73.949...\nNaN\n1.0\n0.0\n...\nStation Wagon/Sport Utility Vehicle\nAVENUE X\nE 21 ST\nBike\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2024-10-29T00:00:00.000\n2024-11-03 13:15:00\nBROOKLYN\n11204\n40.61355\n-73.996560\n{'latitude': '40.61355', 'longitude': '-73.996...\n1782 BAY RIDGE PKWY\n0.0\n0.0\n...\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nBus\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2024-10-29T00:00:00.000\n2024-11-03 07:10:00\nMANHATTAN\n10002\n40.71327\n-73.981910\n{'latitude': '40.71327', 'longitude': '-73.981...\n356 MADISON ST\n0.0\n0.0\n...\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\nimport pandas as pd\nimport sqlite3\nimport os\n# Save to CSV\ndata_DIR = \"output/datasets\"\nos.makedirs(data_DIR, exist_ok=True)\ndf.to_csv('output/datasets/dataset.csv', index=False)\n\n# Save to json\ndf.to_json('output/datasets/dataset.json')\n\n#save to sqlite\ndata=df.copy()\nfor column in data.columns:\n    # Check if any entry in the column is a dictionary\n    if data[column].apply(lambda x: isinstance(x, dict)).any():\n        print(f\"Column {column} contains dictionary objects.\")\nimport json\n\ndef serialize_dicts(x):\n    if isinstance(x, dict):\n        return json.dumps(x)\n    return x\n\n# Apply serialization to all columns that might contain dictionary objects\nfor column in data.columns:\n    data[column] = data[column].apply(serialize_dicts)\n\nimport sqlite3\n\nconn = sqlite3.connect('output/datasets/dataset.db')\ntry:\n    data.to_sql('dataset', conn, if_exists='replace', index=False)\n    print(\"Data saved successfully.\")\nexcept Exception as e:\n    print(f\"An error occurred while saving to SQLite: {e}\")\nfinally:\n    conn.close()\n\nColumn location contains dictionary objects.\nData saved successfully.",
    "crumbs": [
      "Home",
      "Data Wrangling: retrieval",
      "3: From raw to tabular data"
    ]
  },
  {
    "objectID": "09_summary_stats.html",
    "href": "09_summary_stats.html",
    "title": "9: Summary statistics",
    "section": "",
    "text": "In analyzing the New York City Motor Vehicle Collision dataset, several summary statistics can provide insights into the nature and impact of collisions across the city. Here are some meaningful summary statistics for this dataset we are planing to explore:\n\nAverage Number of Collisions Per Day: This statistic helps understand the daily frequency of collisions, providing a baseline for identifying days with unusually high or low numbers of incidents. It’s a key indicator of overall traffic safety.\nMedian Number of Persons Injured in Collisions: The median gives a better sense of the typical collision severity by showing the middle value of injuries in all reported collisions. It’s less influenced by extreme values than the mean, making it a reliable measure of typical outcomes.\nPercentiles for Number of Fatalities in Collisions Percentiles (such as the 90th, 95th, and 99th) for fatalities can help identify the severity distribution of the most lethal collisions. Understanding the tail of this distribution is crucial for targeted interventions on the most dangerous incidents.\nAverage Number of Pedestrians, Cyclists, and Motorists Involved in Collisions Breaking down the average number of pedestrians, cyclists, and motorists involved in collisions can highlight which road users are most at risk. This can inform targeted safety campaigns or infrastructure improvements.\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\n\n# Load the dataset\ndata_path = 'output/datasets/dataset_cleaned.csv'\ndata = pd.read_csv(data_path)\n\n\n# Display basic summary statistics for numerical columns\nsummary_stats = data.describe()\n\n# Displaying the results\nprint(summary_stats)\n\n           latitude     longitude  number_of_persons_injured  \\\ncount  1.760105e+06  1.760105e+06               1.760089e+06   \nmean   4.072425e+01 -7.391972e+01               3.136728e-01   \nstd    7.916310e-02  8.586757e-02               6.987703e-01   \nmin    4.049895e+01 -7.425496e+01               0.000000e+00   \n25%    4.066829e+01 -7.397446e+01               0.000000e+00   \n50%    4.072096e+01 -7.392689e+01               0.000000e+00   \n75%    4.077010e+01 -7.386693e+01               0.000000e+00   \nmax    4.091288e+01 -7.370055e+01               4.300000e+01   \n\n       number_of_persons_killed  number_of_pedestrians_injured  \\\ncount              1.760077e+06                   1.760105e+06   \nmean               1.474367e-03                   5.923056e-02   \nstd                4.044532e-02                   2.495776e-01   \nmin                0.000000e+00                   0.000000e+00   \n25%                0.000000e+00                   0.000000e+00   \n50%                0.000000e+00                   0.000000e+00   \n75%                0.000000e+00                   0.000000e+00   \nmax                8.000000e+00                   2.700000e+01   \n\n       number_of_pedestrians_killed  number_of_cyclist_injured  \\\ncount                  1.760105e+06               1.760105e+06   \nmean                   7.397286e-04               2.834831e-02   \nstd                    2.770540e-02               1.679093e-01   \nmin                    0.000000e+00               0.000000e+00   \n25%                    0.000000e+00               0.000000e+00   \n50%                    0.000000e+00               0.000000e+00   \n75%                    0.000000e+00               0.000000e+00   \nmax                    6.000000e+00               4.000000e+00   \n\n       number_of_cyclist_killed  number_of_motorist_injured  \\\ncount              1.760105e+06                1.760105e+06   \nmean               1.170385e-04                2.219288e-01   \nstd                1.087019e-02                6.577966e-01   \nmin                0.000000e+00                0.000000e+00   \n25%                0.000000e+00                0.000000e+00   \n50%                0.000000e+00                0.000000e+00   \n75%                0.000000e+00                0.000000e+00   \nmax                2.000000e+00                4.300000e+01   \n\n       number_of_motorist_killed  collision_id  \ncount               1.760105e+06  1.760105e+06  \nmean                5.914420e-04  3.335375e+06  \nstd                 2.648232e-02  1.389584e+06  \nmin                 0.000000e+00  1.579000e+03  \n25%                 0.000000e+00  3.260377e+06  \n50%                 0.000000e+00  3.764076e+06  \n75%                 0.000000e+00  4.239331e+06  \nmax                 4.000000e+00  4.721095e+06  \n\n\n\nmean_injuries = data['number_of_persons_injured'].mean()\nmedian_injuries = data['number_of_persons_injured'].median()\nprint(\"Mean number of persons injured:\", mean_injuries)\nprint(\"Median number of persons injured:\", median_injuries)\n\nMean number of persons injured: 0.31367277450174397\nMedian number of persons injured: 0.0\n\n\n\nimport plotly.express as px\n\n# Plotting the number of persons injured in each incident\nfig = px.histogram(data, x='number_of_persons_injured', title='Distribution of Persons Injured per Incident')\nfig.show()\n\n                                                \n\n\n\n# Plotting incidents over time, assuming 'date/time' is properly formatted and cleaned\nfig_time = px.histogram(data, x='date/time', title='Distribution of Incidents Over Time')\nfig_time.show()\n\n                                                \n\n\nAverage Number of Collisions Per Day: This statistic helps understand the daily frequency of collisions, providing a baseline for identifying days with unusually high or low numbers of incidents. It’s a key indicator of overall traffic safety.\n\ndata['date/time'] = pd.to_datetime(data['date/time'], errors='coerce')\nprint(data['date/time'].dtype)\nimport pandas as pd\n\n# Assuming 'data' has been loaded and 'date/time' converted to datetime\n# Group by date component of 'date/time'\ndaily_collisions = data.groupby(data['date/time'].dt.date).size()\n\n# Calculate the average number of collisions per day\naverage_collisions_per_day = daily_collisions.mean()\nprint(\"Average Number of Collisions Per Day:\", average_collisions_per_day)\n\ndatetime64[ns]\nAverage Number of Collisions Per Day: 425.55730174081236\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(daily_collisions.index, daily_collisions, marker='o', linestyle='-')\nplt.title('Daily Collisions Over Time')\nplt.xlabel('Date')\nplt.ylabel('Number of Collisions')\nplt.grid(True)\nplt.xticks(rotation=45)  # Rotate date labels for better readability\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMedian Number of Persons Injured in Collisions: The median gives a better sense of the typical collision severity by showing the middle value of injuries in all reported collisions. It’s less influenced by extreme values than the mean, making it a reliable measure of typical outcomes.\n\nmedian_injuries = data['number_of_persons_injured'].median()\nprint(\"Median Number of Persons Injured in Collisions:\", median_injuries)\n\nMedian Number of Persons Injured in Collisions: 0.0\n\n\nPercentiles for Number of Fatalities in Collisions Percentiles (such as the 90th, 95th, and 99th) for fatalities can help identify the severity distribution of the most lethal collisions. Understanding the tail of this distribution is crucial for targeted interventions on the most dangerous incidents.\n\nfatalities_percentiles = data['number_of_persons_killed'].quantile([0.90, 0.95, 0.99])\nprint(\"Fatalities at 90th, 95th, and 99th Percentiles:\", fatalities_percentiles)\n\nFatalities at 90th, 95th, and 99th Percentiles: 0.90    0.0\n0.95    0.0\n0.99    0.0\nName: number_of_persons_killed, dtype: float64\n\n\nAverage Number of Pedestrians, Cyclists, and Motorists Involved in Collisions Breaking down the average number of pedestrians, cyclists, and motorists involved in collisions can highlight which road users are most at risk. This can inform targeted safety campaigns or infrastructure improvements.\n\navg_pedestrians = data['number_of_pedestrians_injured'].mean()\navg_cyclists = data['number_of_cyclist_injured'].mean()\navg_motorists = data['number_of_motorist_injured'].mean()\n\nprint(\"Average Number of Pedestrians Involved in Collisions:\", avg_pedestrians)\nprint(\"Average Number of Cyclists Involved in Collisions:\", avg_cyclists)\nprint(\"Average Number of Motorists Involved in Collisions:\", avg_motorists)\n\nAverage Number of Pedestrians Involved in Collisions: 0.05923055726789027\nAverage Number of Cyclists Involved in Collisions: 0.02834830876567023\nAverage Number of Motorists Involved in Collisions: 0.22192880538376972\n\n\n\navg_data = {\n    'Category': ['Pedestrians', 'Cyclists', 'Motorists'],\n    'Average Involved': [avg_pedestrians, avg_cyclists, avg_motorists]\n}\navg_df = pd.DataFrame(avg_data)\n\nfig_bar = px.bar(avg_df, x='Category', y='Average Involved', title='Average Number of Pedestrians, Cyclists, and Motorists Involved in Collisions')\nfig_bar.show()",
    "crumbs": [
      "Home",
      "Data Wrangling: Analysis",
      "9: Summary statistics"
    ]
  },
  {
    "objectID": "20_reproducibility.html",
    "href": "20_reproducibility.html",
    "title": "20: Reproducibility",
    "section": "",
    "text": "Using below commands I have automatically generated a requirements.txt file for this project, using this file we can install all the required packages to run the codes. pip3 install pipreqs pip3 install pip-tools pipreqs –savepath=requirements.in && pip-compile\ninstall all the libraries from below command\npip install -r requirements.txt\nthen run all qmds in the order to reproduce the results",
    "crumbs": [
      "Home",
      "Project and software development",
      "20: Reproducibility"
    ]
  },
  {
    "objectID": "02_automated_retrieval.html",
    "href": "02_automated_retrieval.html",
    "title": "2: Retrieval of raw data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pydeck as pdk\nimport plotly.express as px\nfrom sodapy import Socrata\nimport pandas as pd\n# import pandahelper.reports as ph\nfrom datetime import datetime\nimport urllib\n\n\ndef dataloading():\n    query = \"\"\"\n        SELECT\n            *\n        WHERE\n            crash_date &gt;= '2018'\n        ORDER BY\n            crash_date DESC\n        \n        \"\"\"\n\n    # format values in the url, easier to read\n    safe_string = urllib.parse.quote_plus(query)\n    print(safe_string)\n\n    # compose url\n    url = 'https://data.cityofnewyork.us/resource/h9gi-nx95.json?$query={}'.format(safe_string)\n    print('url:', url)\n    return url",
    "crumbs": [
      "Home",
      "Data Wrangling: retrieval",
      "2: Retrieval of raw data"
    ]
  },
  {
    "objectID": "15_commit.html",
    "href": "15_commit.html",
    "title": "15: Git commit history",
    "section": "",
    "text": "See https://bellecp.github.io/597-Data-Wrangling-Spring-2024/project.html#clear-commit-history. Describe your contributions below.\n\n\nfrom IPython.display import Image\nImage(filename='commit.png')",
    "crumbs": [
      "Home",
      "Project and software development",
      "15: Git commit history"
    ]
  },
  {
    "objectID": "06_test_quality.html",
    "href": "06_test_quality.html",
    "title": "6: Test data quality",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport pydeck as pdk\nimport plotly.express as px\nfrom sodapy import Socrata\nimport pandas as pd\n# import pandahelper.reports as ph\nfrom datetime import datetime\nimport urllib\n\n\ndateparse = lambda x: pd.to_datetime(x, format='%Y-%m-%d %H:%M:%S')\ndata = pd.read_csv('output/datasets/dataset_cleaned.csv', parse_dates=['date/time'], date_parser=dateparse)\n\n# Check the dtype to ensure conversion\nprint(data['date/time'].dtype)\n\ndatetime64[ns]\n\n\n\ndata.columns\n\nIndex(['crash_date', 'crash_time', 'borough', 'zip_code', 'latitude',\n       'longitude', 'location', 'on_street_name', 'off_street_name',\n       'number_of_persons_injured', 'number_of_persons_killed',\n       'number_of_pedestrians_injured', 'number_of_pedestrians_killed',\n       'number_of_cyclist_injured', 'number_of_cyclist_killed',\n       'number_of_motorist_injured', 'number_of_motorist_killed',\n       'contributing_factor_vehicle_1', 'contributing_factor_vehicle_2',\n       'collision_id', 'vehicle_type_code1', 'vehicle_type_code2',\n       'cross_street_name', 'contributing_factor_vehicle_3',\n       'contributing_factor_vehicle_4', 'vehicle_type_code_3',\n       'vehicle_type_code_4', 'contributing_factor_vehicle_5',\n       'vehicle_type_code_5', 'date/time'],\n      dtype='object')\n\n\n\nVerify No Missing Values in Key Columns:\n\nassert not data['latitude'].isnull().any(), \"Latitude contains null values\"\nassert not data['longitude'].isnull().any(), \"Longitude contains null values\"\nassert not data['date/time'].isnull().any(), \"Date/time contains null values\"\n\n\n\nCheck for Reasonable Number of Rows:\n\nassert len(data) &gt; 0, \"No rows found after cleaning\"\nassert len(data) &gt; 100, \"Data might be too limited\"\n\n\n\nTest Data Type Integrity:\n\nassert pd.api.types.is_numeric_dtype(data['number_of_persons_injured']), \"Number of persons injured is not numeric\"\nassert pd.api.types.is_datetime64_any_dtype(data['date/time']), \"Date/time is not a datetime type\"\n\n\n\nGeographical Bounds Check:\n\nassert data['latitude'].between(40.4, 41.0).all(), \"Latitude out of NYC bounds\"\nassert data['longitude'].between(-74.3, -73.7).all(), \"Longitude out of NYC bounds\"\n\n\n\nPytest(Created a new file test_06.py for pytest using below code)\n\nimport pytest\nimport pandas as pd\nfrom functions import scrub_data\n\nimport warnings\n\n# Suppress specific deprecation warnings from libraries you do not control\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"plotly.*\")\n\n\n@pytest.fixture\ndef data():\n    # Load the DataFrame from a CSV or define it directly\n    df = pd.read_csv('output/datasets/dataset.csv',low_memory=False)\n    # Apply any necessary preprocessing you typically use before tests\n    df = scrub_data(df)  # Assuming 'scrub_data' is a function you use to clean data\n    return df\n\n\ndef test_no_missing_values(data):  # 'data' fixture is used here\n    for column in ['latitude', 'longitude', 'date/time']:\n        assert not data[column].isnull().any(), f\"{column} contains null values\"\n\ndef test_reasonable_number_of_rows(data):\n    assert len(data) &gt; 100, \"Data might be too limited after cleaning\"\n\ndef test_data_types(data):\n    assert pd.api.types.is_numeric_dtype(data['number_of_persons_injured']), \"Number of persons injured is not numeric\"\n    assert pd.api.types.is_datetime64_any_dtype(data['date/time']), \"Date/time is not a datetime type\"\n\ndef test_geographical_bounds(data):\n    assert data['latitude'].between(40.4, 41.0).all(), \"Latitude out of NYC bounds\"\n    assert data['longitude'].between(-74.3, -73.7).all(), \"Longitude out of NYC bounds\"",
    "crumbs": [
      "Home",
      "Data Wrangling: enrichment/cleaning",
      "6: Test data quality"
    ]
  },
  {
    "objectID": "18_continuous_integration.html",
    "href": "18_continuous_integration.html",
    "title": "18: cont. integration",
    "section": "",
    "text": "# when we push, pull or make any commits this workflow will run and runs the pytest and gets the latest data, dataset will get upto date\n#and figures will get updated\n\n''' \nname: Python application, Quarto Test, and Publish All Documents\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  build-and-publish:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - uses: quarto-dev/quarto-actions/setup@v2\n    - run: |\n        quarto --version\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: '3.8'\n    - name: Install Python dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Find all Quarto documents and publish\n      run: |\n        for file in $(find . -name '*.qmd'); do\n          quarto render $file\n          quarto publish $file --to github-pages --repo stats-at-Rutgers/group-project-data_dudes_13\n        done\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Test with pytest\n      run: pytest\n     '''\n\n\" \\nname: Python application, Quarto Test, and Publish All Documents\\n\\non:\\n  push:\\n    branches: [ main ]\\n  pull_request:\\n    branches: [ main ]\\n\\njobs:\\n  build-and-publish:\\n    runs-on: ubuntu-latest\\n    steps:\\n    - uses: actions/checkout@v2\\n    - uses: quarto-dev/quarto-actions/setup@v2\\n    - run: |\\n        quarto --version\\n    - name: Set up Python\\n      uses: actions/setup-python@v2\\n      with:\\n        python-version: '3.8'\\n    - name: Install Python dependencies\\n      run: |\\n        python -m pip install --upgrade pip\\n        pip install -r requirements.txt\\n\\n    - name: Find all Quarto documents and publish\\n      run: |\\n        for file in $(find . -name '*.qmd'); do\\n          quarto render $file\\n          quarto publish $file --to github-pages --repo stats-at-Rutgers/group-project-data_dudes_13\\n        done\\n      env:\\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n\\n    - name: Test with pytest\\n      run: pytest\\n     \"",
    "crumbs": [
      "Home",
      "Project and software development",
      "18: cont. integration"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Team 13",
    "section": "",
    "text": "The “NYC Vehicle Collisions” investigates the patterns of vehicle collisions in New York City, with a special focus on accidents involving pedestrians and cyclists and collisions. Using NYPD collision data and precinct shapefiles, the project analyzes where and when these incidents occur most frequently, presenting findings through maps and other visualizations including interactivity. The goal is to identify high-risk areas and suggest improvements. # Team members\nGroup member (netid and name).\n\nBhavana Vemula bv234\nLayakishore Reddy Desireddy ld786\nVikram Sriram Yasaswi Sagaram vs865",
    "crumbs": [
      "Home",
      "Team 13"
    ]
  },
  {
    "objectID": "14_poster.html",
    "href": "14_poster.html",
    "title": "14: Poster",
    "section": "",
    "text": "{{}}\n\nfrom IPython.display import Image\n\nImage(filename='Poster_Group_13.png')",
    "crumbs": [
      "Home",
      "Data Wrangling: Communicating results",
      "14: Poster"
    ]
  },
  {
    "objectID": "14_poster.html#downloading-poster.pdf-file",
    "href": "14_poster.html#downloading-poster.pdf-file",
    "title": "14: Poster",
    "section": "",
    "text": "{{}}\n\nfrom IPython.display import Image\n\nImage(filename='Poster_Group_13.png')",
    "crumbs": [
      "Home",
      "Data Wrangling: Communicating results",
      "14: Poster"
    ]
  },
  {
    "objectID": "19_python_package.html",
    "href": "19_python_package.html",
    "title": "19: python package (optional)",
    "section": "",
    "text": "https://bellecp.github.io/597-Data-Wrangling-Spring-2024/project.html#optional-build-your-own-external-python-package-and-use-this-package-as-a-dependency-in-the-main-project. Describe your contributions below.",
    "crumbs": [
      "Home",
      "Project and software development",
      "19: python package (optional)"
    ]
  },
  {
    "objectID": "04_enrichment.html",
    "href": "04_enrichment.html",
    "title": "4: Data enrichment",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport pydeck as pdk\nimport plotly.express as px\nfrom sodapy import Socrata\nimport pandas as pd\n# import pandahelper.reports as ph\nfrom datetime import datetime\nimport urllib\n\n\ndata = pd.read_csv('output/datasets/dataset.csv')\n\n\nimport pandas as pd\n\ndef scrub_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    # Parse Date and Time\n    df['crash_date'] = pd.to_datetime(df['crash_date']).dt.date  # Ensures only the date part is considered\n\n    # Handling crash_time to ensure only time is considered\n    # Assuming 'crash_time' might also include full datetime, we need to isolate the time part.\n    if df['crash_time'].str.contains(':').all():  # Simple check if format includes hours and minutes\n        df['crash_time'] = pd.to_datetime(df['crash_time'], errors='coerce').dt.time\n    else:\n        df['crash_time'] = pd.to_datetime(df['crash_time'], format='%H:%M', errors='coerce').dt.time\n\n    # Combine date and time into a single datetime column\n    df['date/time'] = pd.to_datetime(df['crash_date'].astype(str) + ' ' + df['crash_time'].astype(str))\n\n    # Drop rows with NaN in 'latitude' and 'longitude'\n    df.dropna(subset=['latitude', 'longitude'], inplace=True)\n\n    # Rename columns: make lowercase and replace spaces with underscores\n    df.rename(str.lower, axis='columns', inplace=True)\n    df.columns = df.columns.str.replace(' ', '_')\n\n    # Convert strings to numerical data where applicable\n    numeric_cols = ['number_of_persons_injured', 'number_of_pedestrians_injured', 'number_of_cyclist_injured',\n                    'number_of_motorist_injured', 'number_of_persons_killed', 'number_of_pedestrians_killed',\n                    'number_of_cyclist_killed', 'number_of_motorist_killed']\n    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n    df[['latitude', 'longitude']] = df[['latitude', 'longitude']].apply(pd.to_numeric, errors='coerce')\n\n    # Filter data to include only NYC metro area\n    nyc_bounds = {\n        'latitude_min': 40.4, 'latitude_max': 41.0,\n        'longitude_min': -74.3, 'longitude_max': -73.7\n    }\n    df = df[(df['latitude'].between(nyc_bounds['latitude_min'], nyc_bounds['latitude_max'])) &\n            (df['longitude'].between(nyc_bounds['longitude_min'], nyc_bounds['longitude_max']))]\n    \n    return df\n\n\ndata = scrub_data(data)\n\n\nimport pandas as pd\n\n# Assuming 'data' is your DataFrame\n# Convert date and time into a single datetime column\ntry:\n    # Direct column assignment to handle future deprecation issues\n    data['date/time'] = pd.to_datetime(data['crash_date'].astype(str) + ' ' + data['crash_time'].astype(str))\n    print(\"Datetime conversion successful\")\nexcept Exception as e:\n    print(\"Datetime conversion failed:\", e)\n\n# Define collision categories directly\ndata['total_collisions'] = 1\ndata['serious_collisions'] = ((data['number_of_persons_injured'] &gt; 0) | (data['number_of_persons_killed'] &gt; 0)).astype(int)\ndata['collisions_with_pedestrians_cyclists'] = ((data['number_of_pedestrians_injured'] &gt; 0) |\n                                                (data['number_of_pedestrians_killed'] &gt; 0) |\n                                                (data['number_of_cyclist_injured'] &gt; 0) |\n                                                (data['number_of_cyclist_killed'] &gt; 0)).astype(int)\n\n# Time period categorization\ndata['year'] = data['date/time'].dt.year\ndata['month_year'] = data['date/time'].dt.to_period('M').astype(str)\ndata['week_year'] = data['date/time'].dt.to_period('W').astype(str)\ndata['day_year'] = data['date/time'].dt.to_period('D').astype(str)\n\n# Group and aggregate data\ndata_yearly = data.groupby('year').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\ndata_monthly = data.groupby('month_year').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\ndata_weekly = data.groupby('week_year').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\ndata_daily = data.groupby('day_year').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\nDatetime conversion successful",
    "crumbs": [
      "Home",
      "Data Wrangling: enrichment/cleaning",
      "4: Data enrichment"
    ]
  },
  {
    "objectID": "12_webpage.html",
    "href": "12_webpage.html",
    "title": "12: Webpage",
    "section": "",
    "text": "https://stats-at-rutgers.github.io/group-project-data_dudes_13/13_interactive.html",
    "crumbs": [
      "Home",
      "Data Wrangling: Communicating results",
      "12: Webpage"
    ]
  },
  {
    "objectID": "07_format_conversion.html",
    "href": "07_format_conversion.html",
    "title": "7: Backup in several formats",
    "section": "",
    "text": "dataset is backed up into external service using below code we are saving it to csv, json,sqlite3, these codes are already used in 03 and 04\n\n'''\nimport pandas as pd\nimport sqlite3\nimport os\n# Save to CSV\ndata_DIR = \"output/datasets\"\nos.makedirs(data_DIR, exist_ok=True)\ndf.to_csv('output/datasets/dataset.csv', index=False)\n\n# Save to json\ndf.to_json('output/datasets/dataset.json')\n\n#save to sqlite\ndata=df.copy()\nfor column in data.columns:\n    # Check if any entry in the column is a dictionary\n    if data[column].apply(lambda x: isinstance(x, dict)).any():\n        print(f\"Column {column} contains dictionary objects.\")\nimport json\n\ndef serialize_dicts(x):\n    if isinstance(x, dict):\n        return json.dumps(x)\n    return x\n\n# Apply serialization to all columns that might contain dictionary objects\nfor column in data.columns:\n    data[column] = data[column].apply(serialize_dicts)\n\nimport sqlite3\n\nconn = sqlite3.connect('output/datasets/dataset.db')\ntry:\n    data.to_sql('dataset', conn, if_exists='replace', index=False)\n    print(\"Data saved successfully.\")\nexcept Exception as e:\n    print(f\"An error occurred while saving to SQLite: {e}\")\nfinally:\n    conn.close()\n\n'''\n\n'\\nimport pandas as pd\\nimport sqlite3\\nimport os\\n# Save to CSV\\ndata_DIR = \"output/datasets\"\\nos.makedirs(data_DIR, exist_ok=True)\\ndf.to_csv(\\'output/datasets/dataset.csv\\', index=False)\\n\\n# Save to json\\ndf.to_json(\\'output/datasets/dataset.json\\')\\n\\n#save to sqlite\\ndata=df.copy()\\nfor column in data.columns:\\n    # Check if any entry in the column is a dictionary\\n    if data[column].apply(lambda x: isinstance(x, dict)).any():\\n        print(f\"Column {column} contains dictionary objects.\")\\nimport json\\n\\ndef serialize_dicts(x):\\n    if isinstance(x, dict):\\n        return json.dumps(x)\\n    return x\\n\\n# Apply serialization to all columns that might contain dictionary objects\\nfor column in data.columns:\\n    data[column] = data[column].apply(serialize_dicts)\\n\\nimport sqlite3\\n\\nconn = sqlite3.connect(\\'output/datasets/dataset.db\\')\\ntry:\\n    data.to_sql(\\'dataset\\', conn, if_exists=\\'replace\\', index=False)\\n    print(\"Data saved successfully.\")\\nexcept Exception as e:\\n    print(f\"An error occurred while saving to SQLite: {e}\")\\nfinally:\\n    conn.close()\\n\\n'",
    "crumbs": [
      "Home",
      "Backups",
      "7: Backup in several formats"
    ]
  },
  {
    "objectID": "16_tests.html",
    "href": "16_tests.html",
    "title": "16: Tests",
    "section": "",
    "text": "We have written a test_06.py which tests the quality of dataset, which can be verified with pytest command",
    "crumbs": [
      "Home",
      "Project and software development",
      "16: Tests"
    ]
  },
  {
    "objectID": "08_backup_external_service.html",
    "href": "08_backup_external_service.html",
    "title": "8: External services",
    "section": "",
    "text": "'''\nfrom google.cloud import storage\n\ndef upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n\n    blob.upload_from_filename(source_file_name)\n    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n\n# Usage\nupload_to_gcs('your-bucket-name', '/path/to/your/dataset.csv', 'dataset_backup.csv')\n'''\n\n'\\nfrom google.cloud import storage\\n\\ndef upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\\n    storage_client = storage.Client()\\n    bucket = storage_client.bucket(bucket_name)\\n    blob = bucket.blob(destination_blob_name)\\n\\n    blob.upload_from_filename(source_file_name)\\n    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\\n\\n# Usage\\nupload_to_gcs(\\'your-bucket-name\\', \\'/path/to/your/dataset.csv\\', \\'dataset_backup.csv\\')\\n'\n\n\n\n'''\nfrom azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n\ndef upload_to_azure(blob_service_client, container_name, file_path, blob_name):\n    try:\n        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n        with open(file_path, \"rb\") as data:\n            blob_client.upload_blob(data, overwrite=True)\n            print(f\"File {file_path} uploaded to {blob_name}.\")\n    except Exception as ex:\n        print(f\"An error occurred: {ex}\")\n\n# Initialize a BlobServiceClient\nconnect_str = 'your-azure-connection-string'\nblob_service_client = BlobServiceClient.from_connection_string(connect_str)\n\n# Usage\nupload_to_azure(blob_service_client, 'your-container-name', '/path/to/your/dataset.csv', 'dataset_backup.csv')\n'''\n\n'\\nfrom azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\\n\\ndef upload_to_azure(blob_service_client, container_name, file_path, blob_name):\\n    try:\\n        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\\n        with open(file_path, \"rb\") as data:\\n            blob_client.upload_blob(data, overwrite=True)\\n            print(f\"File {file_path} uploaded to {blob_name}.\")\\n    except Exception as ex:\\n        print(f\"An error occurred: {ex}\")\\n\\n# Initialize a BlobServiceClient\\nconnect_str = \\'your-azure-connection-string\\'\\nblob_service_client = BlobServiceClient.from_connection_string(connect_str)\\n\\n# Usage\\nupload_to_azure(blob_service_client, \\'your-container-name\\', \\'/path/to/your/dataset.csv\\', \\'dataset_backup.csv\\')\\n'",
    "crumbs": [
      "Home",
      "Backups",
      "8: External services"
    ]
  },
  {
    "objectID": "01_document_sources.html",
    "href": "01_document_sources.html",
    "title": "1: Data Sources",
    "section": "",
    "text": "The dataset is sourced from the New York City Open Data platform, specifically provided by the New York City Police Department (NYPD). This platform is a part of the city’s initiative to increase public access to data generated by various departments of NYC. The New York City Police Department (NYPD) is responsible for compiling and maintaining this dataset. The NYPD is the largest and one of the oldest municipal police departments in the United States, with primary responsibilities in law enforcement and investigation within the five boroughs of New York City.\nhttps://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/about_data",
    "crumbs": [
      "Home",
      "Data Wrangling: retrieval",
      "1: Data Sources"
    ]
  },
  {
    "objectID": "01_document_sources.html#source",
    "href": "01_document_sources.html#source",
    "title": "1: Data Sources",
    "section": "",
    "text": "The dataset is sourced from the New York City Open Data platform, specifically provided by the New York City Police Department (NYPD). This platform is a part of the city’s initiative to increase public access to data generated by various departments of NYC. The New York City Police Department (NYPD) is responsible for compiling and maintaining this dataset. The NYPD is the largest and one of the oldest municipal police departments in the United States, with primary responsibilities in law enforcement and investigation within the five boroughs of New York City.\nhttps://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/about_data",
    "crumbs": [
      "Home",
      "Data Wrangling: retrieval",
      "1: Data Sources"
    ]
  },
  {
    "objectID": "01_document_sources.html#format",
    "href": "01_document_sources.html#format",
    "title": "1: Data Sources",
    "section": "Format:",
    "text": "Format:\nThe dataset is available in multiple formats, including CSV for bulk downloads and JSON for API access. The API, provided through the Socrata Open Data API (SODA), allows for real-time data retrieval and querying specific data points based on various parameters such as date, location, and type of collision.",
    "crumbs": [
      "Home",
      "Data Wrangling: retrieval",
      "1: Data Sources"
    ]
  },
  {
    "objectID": "01_document_sources.html#nature-of-the-data",
    "href": "01_document_sources.html#nature-of-the-data",
    "title": "1: Data Sources",
    "section": "Nature of the Data:",
    "text": "Nature of the Data:\nThe dataset contains detailed records of motor vehicle collisions reported to the NYPD. Each record includes information such as date and time of the collision, location (latitude and longitude coordinates, borough, zip code, street names), involved parties (number of persons injured, number of persons killed, number of pedestrians involved, cyclists, and motorists), type of vehicles involved, and contributing factors to the collision.\nThe data covers all reported motor vehicle collisions in New York City since July 2012 and is updated daily to include new incidents. This ensures the dataset is a comprehensive and up-to-date source for analyzing trends in road safety and vehicle collisions within the city.",
    "crumbs": [
      "Home",
      "Data Wrangling: retrieval",
      "1: Data Sources"
    ]
  },
  {
    "objectID": "01_document_sources.html#key-features-of-the-data",
    "href": "01_document_sources.html#key-features-of-the-data",
    "title": "1: Data Sources",
    "section": "Key Features of the Data:",
    "text": "Key Features of the Data:\n\nTemporal Data: Date and time of collisions allow for time-series analysis to identify trends over different periods.\nSpatial Data: Geographic coordinates and location descriptions enable spatial analysis to identify high-risk areas and collision hotspots.\nCategorical Data: Information on the type of vehicles involved and contributing factors to collisions supports categorical analysis for identifying common causes and outcomes.\nQuantitative Data: Counts of injured and killed persons provide quantitative measures to assess the severity and impact of collisions.",
    "crumbs": [
      "Home",
      "Data Wrangling: retrieval",
      "1: Data Sources"
    ]
  },
  {
    "objectID": "11_visualizations.html",
    "href": "11_visualizations.html",
    "title": "11: Visualizations",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport pydeck as pdk\nimport plotly.express as px\nfrom sodapy import Socrata\nimport pandas as pd\n# import pandahelper.reports as ph\nfrom datetime import datetime\nimport urllib\nimport os\ndateparse = lambda x: pd.to_datetime(x, format='%Y-%m-%d %H:%M:%S')\ndata1 = pd.read_csv('output/datasets/dataset_cleaned.csv', parse_dates=['date/time'], date_parser=dateparse)\n\n# Check the dtype to ensure conversion\nprint(data1['date/time'].dtype)\n\ndatetime64[ns]\n\n\n\nimport pandas as pd\n\ndata=data1.copy()\ntry:\n    data.loc[:, 'date/time'] = pd.to_datetime(data['crash_date'].astype(str) + ' ' + data['crash_time'].astype(str))\n    print(\"Datetime conversion successful\")\nexcept Exception as e:\n    print(\"Datetime conversion failed:\", e)\n\n# Define collision categories explicitly with .loc\ndata.loc[:, 'total_collisions'] = 1\ndata.loc[:, 'serious_collisions'] = ((data['number_of_persons_injured'] &gt; 0) | (data['number_of_persons_killed'] &gt; 0)).astype(int)\ndata.loc[:, 'collisions_with_pedestrians_cyclists'] = ((data['number_of_pedestrians_injured'] &gt; 0) |\n                                                      (data['number_of_pedestrians_killed'] &gt; 0) |\n                                                      (data['number_of_cyclist_injured'] &gt; 0) |\n                                                      (data['number_of_cyclist_killed'] &gt; 0)).astype(int)\n\n# Aggregate data for different time periods\ndata.loc[:, 'year'] = data['date/time'].dt.year\ndata.loc[:, 'month_year'] = data['date/time'].dt.to_period('M').astype(str)\ndata.loc[:, 'week_year'] = data['date/time'].dt.to_period('W').astype(str)\ndata.loc[:, 'day_year'] = data['date/time'].dt.to_period('D').astype(str)\n\n# Group and aggregate data\ndata_yearly = data.groupby('year').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\ndata_monthly = data.groupby('month_year').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\ndata_weekly = data.groupby('week_year').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\ndata_daily = data.groupby('day_year').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\nDatetime conversion successful\n\n\n\nimport plotly.graph_objects as go\n\n# Function to create a plot with customizations\ndef create_plot():\n    # Create an empty figure\n    fig = go.Figure()\n\n    # Define initial data (yearly data as default)\n    initial_data = data_yearly\n\n    # Add traces for each type of collision\n    fig.add_trace(go.Bar(\n        x=initial_data['year'], y=initial_data['total_collisions'],\n        name='Total Collisions',\n        marker_color='rgba(55, 83, 109, 0.8)',  # dark blue\n        text=initial_data['total_collisions'],  # Add text on bars\n        textposition='outside'  # Position text outside of bars\n    ))\n    fig.add_trace(go.Bar(\n        x=initial_data['year'], y=initial_data['serious_collisions'],\n        name='Serious Collisions',\n        marker_color='rgba(255, 133, 27, 0.8)',  # bright orange\n        text=initial_data['serious_collisions'],  # Add text on bars\n        textposition='outside'  # Position text outside of bars\n    ))\n    fig.add_trace(go.Bar(\n        x=initial_data['year'], y=initial_data['collisions_with_pedestrians_cyclists'],\n        name='Collisions with Pedestrians and Cyclists',\n        marker_color='rgba(50, 171, 96, 0.8)',  # green\n        text=initial_data['collisions_with_pedestrians_cyclists'],  # Add text on bars\n        textposition='outside'  # Position text outside of bars\n    ))\n\n    # Update layout with dropdown\n    fig.update_layout(\n        title='Traffic Collision Data - Yearly View',\n        xaxis_title='Year',\n        yaxis_title='Number of Collisions',\n        barmode='group',\n        legend_title='Types of Collisions',\n        plot_bgcolor='white',\n        width=1200,\n        height=600,\n        updatemenus=[\n            dict(\n                buttons=list([\n                    dict(args=[{'x': [data_yearly['year'].astype(str)], 'y': [data_yearly[col] for col in ['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists']],\n                               'text': [data_yearly[col] for col in ['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists']]}],\n                         label=\"Yearly\",\n                         method=\"restyle\"),\n                    dict(args=[{'x': [data_monthly['month_year']], 'y': [data_monthly[col] for col in ['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists']],\n                               'text': [data_monthly[col] for col in ['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists']]}],\n                         label=\"Monthly\",\n                         method=\"restyle\"),\n                    dict(args=[{'x': [data_weekly['week_year']], 'y': [data_weekly[col] for col in ['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists']],\n                               'text': [data_weekly[col] for col in ['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists']]}],\n                         label=\"Weekly\",\n                         method=\"restyle\"),\n                    dict(args=[{'x': [data_daily['day_year']], 'y': [data_daily[col] for col in ['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists']],\n                               'text': [data_daily[col] for col in ['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists']]}],\n                         label=\"Daily\",\n                         method=\"restyle\")\n                ]),\n                direction=\"down\",\n                pad={\"r\": 10, \"t\": 10},\n                showactive=True,\n                x=0.1,\n                xanchor=\"left\",\n                y=1.15,\n                yanchor=\"top\"\n            ),\n        ]\n    )\n\n    return fig\nfig = create_plot()\nIMG_DIR = \"output/trends\"\nos.makedirs(IMG_DIR, exist_ok=True)\nfig.write_html(os.path.join(IMG_DIR, \"dataset_vis.html\"))\n# Generate and show the plot\n\n\nfig.show()\n\n                                                \n\n\n\nMonthly Collision Data Across All Years\n\nimport pandas as pd\nimport plotly.express as px\n\n# Load the data\ndata = data1.copy()\n\n# Convert 'date/time' to datetime if necessary\ndata['date/time'] = pd.to_datetime(data['date/time'], errors='coerce')\n\n# Ensure the additional columns are in place\ndata['total_collisions'] = 1\ndata['serious_collisions'] = ((data['number_of_persons_injured'] &gt; 0) | (data['number_of_persons_killed'] &gt; 0)).astype(int)\ndata['collisions_with_pedestrians_cyclists'] = ((data['number_of_pedestrians_injured'] &gt; 0) |\n                                                (data['number_of_pedestrians_killed'] &gt; 0) |\n                                                (data['number_of_cyclist_injured'] &gt; 0) |\n                                                (data['number_of_cyclist_killed'] &gt; 0)).astype(int)\n# Extract month from 'date/time' for grouping\ndata['month'] = data['date/time'].dt.month\n\n# Group by month and sum the collisions\nmonthly_data = data.groupby('month').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\n# Convert 'month' from numbers to names for better readability\nimport calendar\nmonthly_data['month'] = monthly_data['month'].apply(lambda x: calendar.month_abbr[x])\n\n# Define color map for the different collision types\ncolor_map = {\n    'total_collisions': 'pink',  # yellow for total collisions\n    'serious_collisions': 'red',  # Red for serious collisions\n    'collisions_with_pedestrians_cyclists': 'green'  # Green for collisions involving pedestrians/cyclists\n}\n\nimport plotly.express as px\n\nfig = px.bar(monthly_data, \n             x='month', \n             y=['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists'],\n             barmode='group', \n             title='Monthly Collision Data Across All Years',\n             color_discrete_map=color_map)\n\nfig.update_layout(\n    xaxis_title='Month',\n    yaxis_title='Number of Collisions',\n    legend_title='Collision Type'\n)\nIMG_DIR = \"output/trends\"\nos.makedirs(IMG_DIR, exist_ok=True)\nfig.write_html(os.path.join(IMG_DIR, \"monthly_collision.html\"))\nfig.show()\n\n                                                \n\n\n\n\nWeekly Collision Data\n\nimport pandas as pd\nimport plotly.express as px\n\n\n# Extract day of the week from 'date/time' (0=Monday, 6=Sunday)\ndata['day_of_week'] = data['date/time'].dt.dayofweek\n\n# Map integers to day names for better readability\nday_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\ndata['day_of_week'] = data['day_of_week'].map(day_names)\n\n# Group by day of the week and sum the collisions\nweekly_data = data.groupby('day_of_week').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\n# To ensure the days are in the correct order\nweekly_data['day_of_week'] = pd.Categorical(weekly_data['day_of_week'], categories=day_names.values(), ordered=True)\nweekly_data.sort_values('day_of_week', inplace=True)\n\n# Define color map for the different collision types\ncolor_map = {\n    'total_collisions': 'pink',  # pink for total collisions\n    'serious_collisions': 'red',  # Red for serious collisions\n    'collisions_with_pedestrians_cyclists': 'green'  # Green for collisions involving pedestrians/cyclists\n}\n\nfig = px.bar(weekly_data, \n             x='day_of_week', \n             y=['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists'],\n             barmode='group', \n             title='Weekly Collision Data',\n             color_discrete_map=color_map)\n\nfig.update_layout(\n    xaxis_title='Day of the Week',\n    yaxis_title='Number of Collisions',\n    legend_title='Collision Type'\n)\nIMG_DIR = \"output/trends\"\nos.makedirs(IMG_DIR, exist_ok=True)\nfig.write_html(os.path.join(IMG_DIR, \"weekly_collision.html\"))\n\nfig.show()\n\n                                                \n\n\n\n\nHourly Collisions\n\nimport pandas as pd\nimport plotly.express as px\n\n# Ensure 'date/time' is a datetime type\ndata['date/time'] = pd.to_datetime(data['date/time'], errors='coerce')\n\n# Extract the hour from 'date/time'\ndata['hour_of_day'] = data['date/time'].dt.hour\n\n# Group by hour of day and sum the collision types\nhourly_data = data.groupby('hour_of_day').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\n# Ensure hours are sorted (though they should be from the groupby)\nhourly_data.sort_values('hour_of_day', inplace=True)\n# Define color map for the different collision types\ncolor_map = {\n    'total_collisions': 'navy',  # Navy for total collisions\n    'serious_collisions': 'crimson',  # Crimson for serious collisions\n    'collisions_with_pedestrians_cyclists': 'forestgreen'  # Forest green for collisions with pedestrians/cyclists\n}\n\nfig = px.bar(hourly_data, \n             x='hour_of_day', \n             y=['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists'],\n             barmode='group', \n             title='Hourly Collision Data Across All Days',\n             color_discrete_map=color_map)\n\nfig.update_layout(\n    xaxis_title='Hour of the Day',\n    xaxis=dict(tickmode='linear', tick0=0, dtick=1),  # Ensure hourly ticks\n    yaxis_title='Number of Collisions',\n    legend_title='Collision Type'\n)\nIMG_DIR = \"output/trends\"\nos.makedirs(IMG_DIR, exist_ok=True)\nfig.write_html(os.path.join(IMG_DIR, \"hourly_collision.html\"))\nfig.show()\n\n                                                \n\n\n\n\nAcross Seasons\n\nimport pandas as pd\nimport plotly.express as px\n\ndef month_to_season(month):\n    if 3 &lt;= month &lt;= 5:\n        return 'Spring'\n    elif 6 &lt;= month &lt;= 8:\n        return 'Summer'\n    elif 9 &lt;= month &lt;= 11:\n        return 'Fall'\n    else:\n        return 'Winter'  # December to February\n\n# Assuming data has been loaded and 'date/time' converted to datetime\ndata['date/time'] = pd.to_datetime(data['date/time'], errors='coerce')\n\n# Create a 'Season' column based on the month of 'date/time'\ndata['Season'] = data['date/time'].dt.month.apply(month_to_season)\n\n# Group by 'Season' and aggregate collision data\nseasonal_data = data.groupby('Season').agg({\n    'total_collisions': 'sum',\n    'serious_collisions': 'sum',\n    'collisions_with_pedestrians_cyclists': 'sum'\n}).reset_index()\n\n# Order the seasons logically for presentation\nseason_order = ['Winter', 'Spring', 'Summer', 'Fall']\nseasonal_data['Season'] = pd.Categorical(seasonal_data['Season'], categories=season_order, ordered=True)\nseasonal_data = seasonal_data.sort_values('Season')\n\n# Define color map for the different collision types\ncolor_map = {\n    'total_collisions': 'navy',\n    'serious_collisions': 'crimson',\n    'collisions_with_pedestrians_cyclists': 'forestgreen'\n}\n\n# Plotting with custom colors\nfig = px.bar(seasonal_data, \n             x='Season', \n             y=['total_collisions', 'serious_collisions', 'collisions_with_pedestrians_cyclists'],\n             barmode='group', \n             title='Seasonal Collision Data Across All Years',\n             color_discrete_map=color_map)\n\nfig.update_layout(\n    xaxis_title='Season',\n    yaxis_title='Number of Collisions',\n    legend_title='Collision Type'\n)\nIMG_DIR = \"output/trends\"\nos.makedirs(IMG_DIR, exist_ok=True)\nfig.write_html(os.path.join(IMG_DIR, \"season_collision.html\"))\nfig.show()\n\n                                                \n\n\n\n\nDangerous Single Points\nMOST_SERIOUS = 250 # number of most dangerous locations to identify FATALITY_MULTIPLE = 10 # weight assigned to fatalities relative to injuries when we select 250 of the most dangerous locations\nThe worst single points in NYC are: Where West Fordam Road crosses the Major Deegan Expressway in the Bronx Two coordinates on the Verrazano Bridge A single coordinate on the Whitestone Bridge The intersection of Guy R. Brewer Blvd. with Rockaway Blvd. next to Kennedy Airport Where East 138th Street intersects Bruckner Blvd. and the Bruckner Expressway in the Bronx\nSeveral highways stand out as having multiple single coordinates with high numbers of injuries and deaths. For example: The Belt Parkway near Kennedy Airport in Queens The Bruckner Expressway in the Bronx The Cross Bronx Expressway between the Alexander Hamilton Bridge and 3rd Ave (in the Bronx)\nSeveral non-highway roadways stand out as having multiple dangerous single coordinates. For example: Eastern Parkway in Brooklyn Utica Avenue in Brooklyn Woodhaven Blvd. in Queens Fordham Road in the Bronx 125th Street in Manhattan\n\nimport pandas as pd\nimport folium\nimport os\n\ndata = data.copy()\n# Assuming 'data' is your DataFrame and is already loaded\n# Convert latitude and longitude columns to numeric types\n# Convert latitude and longitude columns to numeric types safely\ndata.loc[:, 'latitude'] = pd.to_numeric(data['latitude'], errors='coerce')\ndata.loc[:, 'longitude'] = pd.to_numeric(data['longitude'], errors='coerce')\n\n# Sum up injuries and fatalities using .loc for safe assignment\ndata.loc[:, 'injured'] = data['number_of_persons_injured'] + data['number_of_pedestrians_injured'] + \\\n                         data['number_of_cyclist_injured'] + data['number_of_motorist_injured']\ndata.loc[:, 'killed'] = data['number_of_persons_killed'] + data['number_of_pedestrians_killed'] + \\\n                        data['number_of_cyclist_killed'] + data['number_of_motorist_killed']\n\n# Proceed with the rest of your code\n\n\nMOST_SERIOUS = 250  # number of most dangerous locations to identify\nFATALITY_MULTIPLE = 10  # weight assigned to fatalities relative to injuries\n\n\n\n# Group data by location and sum the results\ndangerous = data.groupby(['latitude', 'longitude'])[['injured', 'killed']].sum()\ndangerous['danger'] = dangerous['injured'] + (dangerous['killed'] * FATALITY_MULTIPLE)\ndangerous = dangerous.reset_index()\nmost_dangerous = dangerous.sort_values(by=\"danger\", ascending=False).head(MOST_SERIOUS)\n\n# Create Folium map\nNYC_MAP_CENTER = (40.73, -73.92)  # center of the map\ndanger_map = folium.Map(location=NYC_MAP_CENTER, zoom_start=10, tiles='OpenStreetMap')\n\n# Add markers to the map\nfor index, row in most_dangerous.iterrows():\n    info_str = f\"INJURED: {int(row['injured'])}\\nKILLED: {int(row['killed'])}\"\n    folium.Marker(\n        location=(row['latitude'], row['longitude']),\n        icon=folium.Icon(color=\"red\", icon=\"fa-exclamation-triangle\", prefix='fa'),\n        tooltip=info_str,\n        popup=info_str\n    ).add_to(danger_map)\n\n# Ensure the output directory exists\nIMG_DIR = \"output/hotspots\"\nos.makedirs(IMG_DIR, exist_ok=True)\ndanger_map.save(os.path.join(IMG_DIR, \"points_serious_map.html\"))\n\n# If running in a Jupyter notebook, display the map\ndanger_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nDangerous Single Points for Pedestrians and Cyclists\n\n# Assuming 'data' is already loaded and has columns for pedestrian and cyclist injuries and fatalities\ndata = data1.copy()\ndata['pedestrian_injured'] = pd.to_numeric(data['number_of_pedestrians_injured'], errors='coerce').fillna(0).astype(int)\ndata['cyclist_injured'] = pd.to_numeric(data['number_of_cyclist_injured'], errors='coerce').fillna(0).astype(int)\ndata['pedestrian_killed'] = pd.to_numeric(data['number_of_pedestrians_killed'], errors='coerce').fillna(0).astype(int)\ndata['cyclist_killed'] = pd.to_numeric(data['number_of_cyclist_killed'], errors='coerce').fillna(0).astype(int)\n\nMOST_NON_MOTORIST_SERIOUS = 250  # number of most dangerous locations to identify\nFATALITY_MULTIPLE = 10  # weight assigned to fatalities relative to injuries\n\n# Group data by location and sum the results for non-motorists\ndangerous_non_motor = data.groupby(['latitude', 'longitude'])[\n    ['pedestrian_injured', 'cyclist_injured', 'pedestrian_killed', 'cyclist_killed']\n].sum()\ndangerous_non_motor['danger'] = (\n    dangerous_non_motor['pedestrian_injured']\n    + dangerous_non_motor['cyclist_injured']\n    + (dangerous_non_motor['pedestrian_killed'] * FATALITY_MULTIPLE)\n    + (dangerous_non_motor['cyclist_killed'] * FATALITY_MULTIPLE)\n)\ndangerous_non_motor = dangerous_non_motor.reset_index()\nmost_dangerous_non_motor = dangerous_non_motor.sort_values(\n    by=\"danger\", ascending=False\n).head(MOST_NON_MOTORIST_SERIOUS)\n\n# Create Folium map\nNYC_MAP_CENTER = (40.73, -73.92)\ndanger_non_motor_map = folium.Map(\n    location=NYC_MAP_CENTER, zoom_start=10, tiles=\"OpenStreetMap\"\n)\n\n# Add markers to the map\nfor index, row in most_dangerous_non_motor.iterrows():\n    info_str = f\"PEDESTRIAN INJURED: {int(row['pedestrian_injured'])}\\nCYCLIST INJURED: {int(row['cyclist_injured'])}\\nPEDESTRIAN KILLED: {int(row['pedestrian_killed'])}\\nCYCLIST KILLED: {int(row['cyclist_killed'])}\"\n    folium.Marker(\n        location=(row['latitude'], row['longitude']),\n        icon=folium.Icon(color=\"red\", icon=\"exclamation-triangle\", prefix=\"fa\"),\n        tooltip=info_str,\n        popup=info_str\n    ).add_to(danger_non_motor_map)\n\n# Ensure the output directory exists\nIMG_DIR = \"output/hotspots\"\nos.makedirs(IMG_DIR, exist_ok=True)\ndanger_non_motor_map.save(os.path.join(IMG_DIR, \"points_non_motor_map.html\"))\n\n# If running in a Jupyter notebook, display the map\ndanger_non_motor_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nFatal Collisions\nZoom in to see serious collisions and the date, time, number injured, and number killed\n\nimport pandas as pd\nimport folium\nimport os\nfrom folium.plugins import FastMarkerCluster\n\ndata = data1.copy()  # Assuming data is already loaded into 'data'\n\n# Define fatal collisions as those with at least one fatality and valid latitude and longitude\nfatal = data[(data['number_of_persons_killed'] &gt; 0) & (data['latitude'].notna()) & (data['longitude'].notna())]\n\n# Prepare data for the map\nmap_data = fatal[['latitude', 'longitude', 'crash_date', 'crash_time', 'number_of_persons_injured', 'number_of_persons_killed']].copy()\nmap_data.loc[:, 'crash_date'] = map_data['crash_date'].astype(str)  # Convert dates to string for JavaScript compatibility\nmap_data.loc[:, 'crash_time'] = map_data['crash_time'].astype(str)  # Convert times to string for JavaScript compatibility\n\n# Create a Folium map centered around New York City\nNYC_MAP_CENTER = [40.7128, -74.0060]  # NYC coordinates for centering the map\nfatal_map = folium.Map(location=NYC_MAP_CENTER, zoom_start=10, tiles='OpenStreetMap')\n\n# JavaScript callback for custom markers\njs_callback = \"\"\"\nfunction (row) {\n    var icon = L.AwesomeMarkers.icon({\n        icon: 'fa-exclamation',\n        iconColor: 'white',\n        markerColor: 'red',\n        prefix: 'fa'\n    });\n    var marker = L.marker(new L.LatLng(row[0], row[1]), {icon: icon});\n    var popup = L.popup({maxWidth: '300'}).setContent('&lt;b&gt;Date:&lt;/b&gt; ' + row[2] + '&lt;br&gt;&lt;b&gt;Time:&lt;/b&gt; ' + row[3] + '&lt;br&gt;&lt;b&gt;People Injured:&lt;/b&gt; ' + row[4] + '&lt;br&gt;&lt;b&gt;People Killed:&lt;/b&gt; ' + row[5]);\n    marker.bindPopup(popup);\n    return marker;\n}\n\"\"\"\n# Add markers to the map using FastMarkerCluster for efficiency\nFastMarkerCluster(data=map_data.values.tolist(), callback=js_callback).add_to(fatal_map)\n\n# Directory for saving the output map\nIMG_DIR = \"output/hotspots\"\nos.makedirs(IMG_DIR, exist_ok=True)\nfatal_map.save(os.path.join(IMG_DIR, \"fatal_map.html\"))\n\n# Display the map if in a Jupyter notebook (optional)\nfatal_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCollisions with Pedestrians and Cyclists 2018 through 2023\n\nimport pandas as pd\nimport folium\nfrom folium.plugins import FastMarkerCluster\nimport os\nfrom datetime import datetime\n\ndata=data1.copy()\n# Assuming data is already loaded into 'data'\n# Filter data for the years 2018 through 2021\nstart = datetime(year=2018, month=1, day=1)\nend = datetime(year=2024, month=1, day=1)\ndata['crash_date'] = pd.to_datetime(data['crash_date'])  # Ensure 'crash_date' is datetime type\ncrashes_18_21 = data[data['crash_date'].between(start, end, inclusive='left')]\n\n# Define collisions involving pedestrians and cyclists\nnon_motorist_18_21 = crashes_18_21[\n    (data['number_of_pedestrians_injured'] &gt; 0) |\n    (data['number_of_cyclist_injured'] &gt; 0) |\n    (data['number_of_pedestrians_killed'] &gt; 0) |\n    (data['number_of_cyclist_killed'] &gt; 0)\n]\n\n# Prepare map data\nnon_motor_map_data = non_motorist_18_21[[\n    'latitude', 'longitude', 'crash_date', 'crash_time',\n    'number_of_persons_injured', 'number_of_pedestrians_injured', 'number_of_cyclist_injured',\n    'number_of_persons_killed', 'number_of_pedestrians_killed', 'number_of_cyclist_killed'\n]]\nnon_motor_map_data['crash_date'] = non_motor_map_data['crash_date'].dt.strftime('%Y-%m-%d')  # Format date\nnon_motor_map_data['crash_time'] = non_motor_map_data['crash_time'].astype(str)  # Ensure time is string\n\n# Create a Folium map centered around New York City\nNYC_MAP_CENTER = [40.7128, -74.0060]  # NYC coordinates for centering the map\nnon_motor_map_18_21 = folium.Map(location=NYC_MAP_CENTER, zoom_start=12, tiles='OpenStreetMap')\n\n# JavaScript callback for custom markers\njs_callback = \"\"\"\nfunction (row) {\n    var icon = L.AwesomeMarkers.icon({\n        icon: 'fa-exclamation',\n        iconColor: 'white',\n        markerColor: 'red',\n        prefix: 'fa'\n    });\n    var marker = L.marker(new L.LatLng(row[0], row[1]), {icon: icon});\n    var popup = L.popup({maxWidth: '300'}).setContent('&lt;b&gt;Date:&lt;/b&gt; ' + row[2] + '&lt;br&gt;&lt;b&gt;Time:&lt;/b&gt; ' + row[3] + '&lt;br&gt;&lt;b&gt;Total Injured:&lt;/b&gt; ' + row[4] + '&lt;br&gt;&lt;b&gt;Pedestrians Injured:&lt;/b&gt; ' + row[5] + '&lt;br&gt;&lt;b&gt;Cyclists Injured:&lt;/b&gt; ' + row[6] + '&lt;br&gt;&lt;b&gt;Total Killed:&lt;/b&gt; ' + row[7] + '&lt;br&gt;&lt;b&gt;Pedestrians Killed:&lt;/b&gt; ' + row[8] + '&lt;br&gt;&lt;b&gt;Cyclists Killed:&lt;/b&gt; ' + row[9]);\n    marker.bindPopup(popup);\n    return marker;\n}\n\"\"\"\nfolium.plugins.FastMarkerCluster(data=non_motor_map_data.values.tolist(), callback=js_callback).add_to(non_motor_map_18_21)\n\n# Save the map to a file\nIMG_DIR = \"output/hotspots\"\nos.makedirs(IMG_DIR, exist_ok=True)\nnon_motor_map_18_21.save(os.path.join(IMG_DIR, \"non_motor_map_18_21.html\"))\n\n# Display the map if in a Jupyter notebook\nnon_motor_map_18_21\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCollisions with Pedestrians and Cyclists in 2024(Latest)\n\nimport pandas as pd\nimport folium\nfrom folium.plugins import FastMarkerCluster\nimport os\nfrom datetime import datetime\ndata=data1.copy()\n# Assuming data is already loaded into 'data'\n# Filter data for the years 2022 through 2024\nstart = datetime(year=2024, month=1, day=1)\nend = datetime(year=2025, month=1, day=1)  # Include up to the end of 2024\ndata['crash_date'] = pd.to_datetime(data['crash_date'])  # Ensure 'crash_date' is datetime type\ncrashes_22_24 = data[data['crash_date'].between(start, end, inclusive='left')]\n\n# Define collisions involving pedestrians and cyclists\nnon_motorist_22_24 = crashes_22_24[\n    (data['number_of_pedestrians_injured'] &gt; 0) |\n    (data['number_of_cyclist_injured'] &gt; 0) |\n    (data['number_of_pedestrians_killed'] &gt; 0) |\n    (data['number_of_cyclist_killed'] &gt; 0)\n]\n\n# Prepare map data\nnon_motor_map_data = non_motorist_22_24[[\n    'latitude', 'longitude', 'crash_date', 'crash_time',\n    'number_of_persons_injured', 'number_of_pedestrians_injured', 'number_of_cyclist_injured',\n    'number_of_persons_killed', 'number_of_pedestrians_killed', 'number_of_cyclist_killed'\n]]\nnon_motor_map_data['crash_date'] = non_motor_map_data['crash_date'].dt.strftime('%Y-%m-%d')  # Format date\nnon_motor_map_data['crash_time'] = non_motor_map_data['crash_time'].astype(str)  # Ensure time is string\n\n# Create a Folium map centered around New York City\nNYC_MAP_CENTER = [40.7128, -74.0060]  # NYC coordinates for centering the map\nnon_motor_map_22_24 = folium.Map(location=NYC_MAP_CENTER, zoom_start=12, tiles='OpenStreetMap')\n\n# JavaScript callback for custom markers\njs_callback = \"\"\"\nfunction (row) {\n    var icon = L.AwesomeMarkers.icon({\n        icon: 'fa-exclamation',\n        iconColor: 'white',\n        markerColor: 'red',\n        prefix: 'fa'\n    });\n    var marker = L.marker(new L.LatLng(row[0], row[1]), {icon: icon});\n    var popup = L.popup({maxWidth: '300'}).setContent('&lt;b&gt;Date:&lt;/b&gt; ' + row[2] + '&lt;br&gt;&lt;b&gt;Time:&lt;/b&gt; ' + row[3] + '&lt;br&gt;&lt;b&gt;Total Injured:&lt;/b&gt; ' + row[4] + '&lt;br&gt;&lt;b&gt;Pedestrians Injured:&lt;/b&gt; ' + row[5] + '&lt;br&gt;&lt;b&gt;Cyclists Injured:&lt;/b&gt; ' + row[6] + '&lt;br&gt;&lt;b&gt;Total Killed:&lt;/b&gt; ' + row[7] + '&lt;br&gt;&lt;b&gt;Pedestrians Killed:&lt;/b&gt; ' + row[8] + '&lt;br&gt;&lt;b&gt;Cyclists Killed:&lt;/b&gt; ' + row[9]);\n    marker.bindPopup(popup);\n    return marker;\n}\n\"\"\"\nfolium.plugins.FastMarkerCluster(data=non_motor_map_data.values.tolist(), callback=js_callback).add_to(non_motor_map_22_24)\n\n# Save the map to a file\nIMG_DIR = \"output/hotspots\"\nos.makedirs(IMG_DIR, exist_ok=True)\nnon_motor_map_22_24.save(os.path.join(IMG_DIR, \"non_motor_map_22_24.html\"))\n\n# Display the map if in a Jupyter notebook\nnon_motor_map_22_24\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nSerious Collisions from 2018 to 2023\n\nimport pandas as pd\nimport folium\nfrom folium.plugins import FastMarkerCluster\nimport os\nfrom datetime import datetime\n\ndata=data1.copy()\n# Assuming data is already loaded into 'data'\n# Define the date range for 2018 through 2021\nstart = datetime(year=2018, month=1, day=1)\nend = datetime(year=2023, month=1, day=1)  # Include up to the end of 2021\ndata['crash_date'] = pd.to_datetime(data['crash_date'])  # Ensure 'crash_date' is datetime type\ncrashes_18_21 = data[data['crash_date'].between(start, end, inclusive='left')]\n\n# Define serious collisions as those with injuries or fatalities\nserious_18_21 = crashes_18_21[\n    (data['number_of_persons_injured'] &gt; 0) | (data['number_of_persons_killed'] &gt; 0)\n]\n\n# Prepare map data\nmap_data = serious_18_21[[\n    'latitude', 'longitude', 'crash_date', 'crash_time',\n    'number_of_persons_injured', 'number_of_persons_killed'\n]]\nmap_data['crash_date'] = map_data['crash_date'].dt.strftime('%Y-%m-%d')  # Format date\nmap_data['crash_time'] = map_data['crash_time'].astype(str)  # Ensure time is string\n\n# Create a Folium map centered around New York City\nNYC_MAP_CENTER = [40.7128, -74.0060]  # NYC coordinates for centering the map\nserious_map_18_21 = folium.Map(location=NYC_MAP_CENTER, zoom_start=12, tiles='OpenStreetMap')\n\n# JavaScript callback for custom markers\njs_callback = \"\"\"\nfunction (row) {\n    var icon = L.AwesomeMarkers.icon({\n        icon: 'fa-exclamation',\n        iconColor: 'white',\n        markerColor: 'red',\n        prefix: 'fa'\n    });\n    var marker = L.marker(new L.LatLng(row[0], row[1]), {icon: icon});\n    var popup = L.popup({maxWidth: '300'}).setContent('&lt;b&gt;Date:&lt;/b&gt; ' + row[2] + '&lt;br&gt;&lt;b&gt;Time:&lt;/b&gt; ' + row[3] + '&lt;br&gt;&lt;b&gt;People Injured:&lt;/b&gt; ' + row[4] + '&lt;br&gt;&lt;b&gt;People Killed:&lt;/b&gt; ' + row[5]);\n    marker.bindPopup(popup);\n    return marker;\n}\n\"\"\"\nfolium.plugins.FastMarkerCluster(data=map_data.values.tolist(), callback=js_callback).add_to(serious_map_18_21)\n\n# Save the map to a file\nIMG_DIR = \"output/hotspots\"\nos.makedirs(IMG_DIR, exist_ok=True)\nserious_map_18_21.save(os.path.join(IMG_DIR, \"serious_map_18_21.html\"))\n\n# Display the map if in a Jupyter notebook\nserious_map_18_21\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nSerious Collisions in 2024(Latest)\n\nimport pandas as pd\nimport folium\nfrom folium.plugins import FastMarkerCluster\nimport os\nfrom datetime import datetime\n\ndata=data1.copy()\n# Assuming data is already loaded into 'data'\n# Define the date range for 2022 through 2024\nstart = datetime(year=2024, month=1, day=1)\nend = datetime(year=2025, month=1, day=1)  # Include up to the end of 2024\ndata['crash_date'] = pd.to_datetime(data['crash_date'])  # Ensure 'crash_date' is datetime type\ncrashes_22_24 = data[data['crash_date'].between(start, end, inclusive='left')]\n\n# Define serious collisions as those with injuries or fatalities\nserious_22_24 = crashes_22_24[\n    (data['number_of_persons_injured'] &gt; 0) | (data['number_of_persons_killed'] &gt; 0)\n]\n\n# Prepare map data\nmap_data = serious_22_24[[\n    'latitude', 'longitude', 'crash_date', 'crash_time',\n    'number_of_persons_injured', 'number_of_persons_killed'\n]]\nmap_data['crash_date'] = map_data['crash_date'].dt.strftime('%Y-%m-%d')  # Format date\nmap_data['crash_time'] = map_data['crash_time'].astype(str)  # Ensure time is string\n\n# Create a Folium map centered around New York City, or adjust the center as needed\nNYC_MAP_CENTER = [40.7128, -74.0060]  # Coordinates for centering the map\nserious_map_22_24 = folium.Map(location=NYC_MAP_CENTER, zoom_start=12, tiles='OpenStreetMap')\n\n# JavaScript callback for custom markers\njs_callback = \"\"\"\nfunction (row) {\n    var icon = L.AwesomeMarkers.icon({\n        icon: 'fa-exclamation',\n        iconColor: 'white',\n        markerColor: 'red',\n        prefix: 'fa'\n    });\n    var marker = L.marker(new L.LatLng(row[0], row[1]), {icon: icon});\n    var popup = L.popup({maxWidth: '300'}).setContent('&lt;b&gt;Date:&lt;/b&gt; ' + row[2] + '&lt;br&gt;&lt;b&gt;Time:&lt;/b&gt; ' + row[3] + '&lt;br&gt;&lt;b&gt;People Injured:&lt;/b&gt; ' + row[4] + '&lt;br&gt;&lt;b&gt;People Killed:&lt;/b&gt; ' + row[5]);\n    marker.bindPopup(popup);\n    return marker;\n}\n\"\"\"\nfolium.plugins.FastMarkerCluster(data=map_data.values.tolist(), callback=js_callback).add_to(serious_map_22_24)\n\n# Save the map to a file\nIMG_DIR = \"output/hotspots\"\nos.makedirs(IMG_DIR, exist_ok=True)\nserious_map_22_24.save(os.path.join(IMG_DIR, \"serious_map_22_24.html\"))\n\n# Display the map if in a Jupyter notebook\nserious_map_22_24\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Home",
      "Data Wrangling: Analysis",
      "11: Visualizations"
    ]
  },
  {
    "objectID": "17_continuous_testing.html",
    "href": "17_continuous_testing.html",
    "title": "17: cont. testing",
    "section": "",
    "text": "# when we push, pull or make any commits this workflow will run and runs the pytest\n\n\n''' \nname: Python application, Quarto Test, and Publish All Documents\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  build-and-publish:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - uses: quarto-dev/quarto-actions/setup@v2\n    - run: |\n        quarto --version\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: '3.8'\n    - name: Install Python dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Find all Quarto documents and publish\n      run: |\n        for file in $(find . -name '*.qmd'); do\n          quarto render $file\n          quarto publish $file --to github-pages --repo stats-at-Rutgers/group-project-data_dudes_13\n        done\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Test with pytest\n      run: pytest\n     '''\n\n\" \\nname: Python application, Quarto Test, and Publish All Documents\\n\\non:\\n  push:\\n    branches: [ main ]\\n  pull_request:\\n    branches: [ main ]\\n\\njobs:\\n  build-and-publish:\\n    runs-on: ubuntu-latest\\n    steps:\\n    - uses: actions/checkout@v2\\n    - uses: quarto-dev/quarto-actions/setup@v2\\n    - run: |\\n        quarto --version\\n    - name: Set up Python\\n      uses: actions/setup-python@v2\\n      with:\\n        python-version: '3.8'\\n    - name: Install Python dependencies\\n      run: |\\n        python -m pip install --upgrade pip\\n        pip install -r requirements.txt\\n\\n    - name: Find all Quarto documents and publish\\n      run: |\\n        for file in $(find . -name '*.qmd'); do\\n          quarto render $file\\n          quarto publish $file --to github-pages --repo stats-at-Rutgers/group-project-data_dudes_13\\n        done\\n      env:\\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n\\n    - name: Test with pytest\\n      run: pytest\\n     \"\n\n\n\n# when we push, pull or make any commits this workflow will run and runs the pytest\n\n''' \nname: Python application, Quarto Test, and Publish All Documents\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  build-and-publish:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - uses: quarto-dev/quarto-actions/setup@v2\n    - run: |\n        quarto --version\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: '3.8'\n    - name: Install Python dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Find all Quarto documents and publish\n      run: |\n        for file in $(find . -name '*.qmd'); do\n          quarto render $file\n          quarto publish $file --to github-pages --repo stats-at-Rutgers/group-project-data_dudes_13\n        done\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Test with pytest\n      run: pytest\n     '''\n\n\" \\nname: Python application, Quarto Test, and Publish All Documents\\n\\non:\\n  push:\\n    branches: [ main ]\\n  pull_request:\\n    branches: [ main ]\\n\\njobs:\\n  build-and-publish:\\n    runs-on: ubuntu-latest\\n    steps:\\n    - uses: actions/checkout@v2\\n    - uses: quarto-dev/quarto-actions/setup@v2\\n    - run: |\\n        quarto --version\\n    - name: Set up Python\\n      uses: actions/setup-python@v2\\n      with:\\n        python-version: '3.8'\\n    - name: Install Python dependencies\\n      run: |\\n        python -m pip install --upgrade pip\\n        pip install -r requirements.txt\\n\\n    - name: Find all Quarto documents and publish\\n      run: |\\n        for file in $(find . -name '*.qmd'); do\\n          quarto render $file\\n          quarto publish $file --to github-pages --repo stats-at-Rutgers/group-project-data_dudes_13\\n        done\\n      env:\\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n\\n    - name: Test with pytest\\n      run: pytest\\n     \"",
    "crumbs": [
      "Home",
      "Project and software development",
      "17: cont. testing"
    ]
  },
  {
    "objectID": "05_cleaning.html",
    "href": "05_cleaning.html",
    "title": "5: Data cleaning",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport pydeck as pdk\nimport plotly.express as px\nfrom sodapy import Socrata\nimport pandas as pd\n# import pandahelper.reports as ph\nfrom datetime import datetime\nimport urllib\n\ndata = pd.read_csv('output/datasets/dataset.csv')\n\nimport pandas as pd\n\ndef scrub_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    # Parse Date and Time\n    df['crash_date'] = pd.to_datetime(df['crash_date']).dt.date  # Ensures only the date part is considered\n\n    # Handling crash_time to ensure only time is considered\n    # Assuming 'crash_time' might also include full datetime, we need to isolate the time part.\n    if df['crash_time'].str.contains(':').all():  # Simple check if format includes hours and minutes\n        df['crash_time'] = pd.to_datetime(df['crash_time'], errors='coerce').dt.time\n    else:\n        df['crash_time'] = pd.to_datetime(df['crash_time'], format='%H:%M', errors='coerce').dt.time\n\n    # Combine date and time into a single datetime column\n    # Ensure correct conversion logic\n    df['date/time'] = pd.to_datetime(df['crash_date'].astype(str) + ' ' + df['crash_time'].astype(str), errors='coerce')\n    df['date/time'] = pd.to_datetime(df['date/time'])\n\n    # Now, check the dtype again\n    print(df['date/time'].dtype)\n\n\n    # Drop rows with NaN in 'latitude' and 'longitude'\n    df.dropna(subset=['latitude', 'longitude'], inplace=True)\n\n    # Rename columns: make lowercase and replace spaces with underscores\n    df.rename(str.lower, axis='columns', inplace=True)\n    df.columns = df.columns.str.replace(' ', '_')\n\n    # Convert strings to numerical data where applicable\n    numeric_cols = ['number_of_persons_injured', 'number_of_pedestrians_injured', 'number_of_cyclist_injured',\n                    'number_of_motorist_injured', 'number_of_persons_killed', 'number_of_pedestrians_killed',\n                    'number_of_cyclist_killed', 'number_of_motorist_killed']\n    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n    df[['latitude', 'longitude']] = df[['latitude', 'longitude']].apply(pd.to_numeric, errors='coerce')\n\n    # Filter data to include only NYC metro area\n    nyc_bounds = {\n        'latitude_min': 40.4, 'latitude_max': 41.0,\n        'longitude_min': -74.3, 'longitude_max': -73.7\n    }\n    df = df[(df['latitude'].between(nyc_bounds['latitude_min'], nyc_bounds['latitude_max'])) &\n            (df['longitude'].between(nyc_bounds['longitude_min'], nyc_bounds['longitude_max']))]\n    \n    return df\n\n\n\ndata = scrub_data(data)\n\ndatetime64[ns]\n\n\n\nimport pandas as pd\nimport sqlite3\nimport os\n# Save to CSV\ndata=data.copy()\ndata_DIR = \"output/datasets\"\nos.makedirs(data_DIR, exist_ok=True)\ndata.to_csv('output/datasets/dataset_cleaned.csv', index=False)\n\n# Save to json\ndata.to_json('output/datasets/dataset_cleaned.json')\n\n#save to sqlite\n\nfor column in data.columns:\n    # Check if any entry in the column is a dictionary\n    if data[column].apply(lambda x: isinstance(x, dict)).any():\n        print(f\"Column {column} contains dictionary objects.\")\nimport json\n\ndef serialize_dicts(x):\n    if isinstance(x, dict):\n        return json.dumps(x)\n    return x\n\n# Apply serialization to all columns that might contain dictionary objects\nfor column in data.columns:\n    data[column] = data[column].apply(serialize_dicts)\n\nimport sqlite3\n\nconn = sqlite3.connect('output/datasets/dataset.db')\ntry:\n    data.to_sql('dataset_cleaned', conn, if_exists='replace', index=False)\n    print(\"Data saved successfully.\")\nexcept Exception as e:\n    print(f\"An error occurred while saving to SQLite: {e}\")\nfinally:\n    conn.close()\n\nData saved successfully.\n\n\n\ndata.columns\n\nIndex(['crash_date', 'crash_time', 'borough', 'zip_code', 'latitude',\n       'longitude', 'location', 'cross_street_name',\n       'number_of_persons_injured', 'number_of_persons_killed',\n       'number_of_pedestrians_injured', 'number_of_pedestrians_killed',\n       'number_of_cyclist_injured', 'number_of_cyclist_killed',\n       'number_of_motorist_injured', 'number_of_motorist_killed',\n       'contributing_factor_vehicle_1', 'contributing_factor_vehicle_2',\n       'contributing_factor_vehicle_3', 'collision_id', 'vehicle_type_code1',\n       'vehicle_type_code2', 'vehicle_type_code_3', 'on_street_name',\n       'off_street_name', 'contributing_factor_vehicle_4',\n       'vehicle_type_code_4', 'contributing_factor_vehicle_5',\n       'vehicle_type_code_5', 'date/time'],\n      dtype='object')\n\n\n\nprint(data['date/time'].dtype)\n\ndatetime64[ns]",
    "crumbs": [
      "Home",
      "Data Wrangling: enrichment/cleaning",
      "5: Data cleaning"
    ]
  },
  {
    "objectID": "10_statistical_ML.html",
    "href": "10_statistical_ML.html",
    "title": "10: Statistical analisys",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport pydeck as pdk\nimport plotly.express as px\nfrom sodapy import Socrata\nimport pandas as pd\n# import pandahelper.reports as ph\nfrom datetime import datetime\nimport urllib\n\n\ndateparse = lambda x: pd.to_datetime(x, format='%Y-%m-%d %H:%M:%S')\ndata = pd.read_csv('output/datasets/dataset_cleaned.csv', parse_dates=['date/time'], date_parser=dateparse)\n\n# Check the dtype to ensure conversion\nprint(data['date/time'].dtype)\n\ndatetime64[ns]\n\n\n\nDangerous Areas\nDangerous areas are clusters of collisions with a high number of injuries and deaths from vehicle collisions. Clusters were identified by tuning a density-based clustering algorithm.\n\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nimport folium\nimport os\nfrom folium.plugins import FastMarkerCluster\n# Assuming 'serious' collisions are those with injuries or fatalities\nserious = data[(data['number_of_persons_injured'] &gt; 0) | (data['number_of_persons_killed'] &gt; 0)].copy()\n\n# Prepare data for clustering\nserious['latitude'] = pd.to_numeric(serious['latitude'], errors='coerce')\nserious['longitude'] = pd.to_numeric(serious['longitude'], errors='coerce')\nserious.dropna(subset=['latitude', 'longitude'], inplace=True)  # Remove any rows without valid lat/long data\n\n# DBSCAN Clustering\ndbscan_model = DBSCAN(eps=0.0008, min_samples=50, metric=\"euclidean\")\nserious['cluster'] = dbscan_model.fit_predict(serious[['latitude', 'longitude']])\n\n# Calculate cluster danger scores based on injuries and fatalities\nSERIOUS_CLUSTERS = 200  # number of most dangerous clusters to identify\nFATALITY_MULTIPLE = 10  # weight assigned to fatalities relative to injuries\n\nserious['injured'] = serious['number_of_persons_injured']\nserious['killed'] = serious['number_of_persons_killed']\ncluster_groupby = serious[serious['cluster'] &gt; -1].groupby('cluster')[['injured', 'killed']].sum()\ncluster_groupby['danger'] = cluster_groupby['injured'] + (cluster_groupby['killed'] * FATALITY_MULTIPLE)\ntop_clusters = cluster_groupby.nlargest(SERIOUS_CLUSTERS, 'danger').index.tolist()\n\n\n# Create a Folium map centered around an average location\nmap_center = [serious['latitude'].mean(), serious['longitude'].mean()]\ncluster_map = folium.Map(location=map_center, zoom_start=12, tiles='OpenStreetMap')\n\n# Select data for mapping and ensure data is appropriate for passing to Folium\ncluster_mask = serious['cluster'].isin(top_clusters)\nfields_to_use = ['latitude', 'longitude', 'crash_date', 'crash_time', 'injured', 'killed', 'cluster']\ncluster_map_data = serious.loc[cluster_mask, fields_to_use]\n\n# Converting date and time to string for JavaScript\ncluster_map_data['crash_date'] = cluster_map_data['crash_date'].astype(str)\ncluster_map_data['crash_time'] = cluster_map_data['crash_time'].astype(str)\n\n# JavaScript callback for custom markers\njs_callback = \"\"\"\nfunction (row) {\n    var icon = L.AwesomeMarkers.icon({\n        icon: 'info-sign',\n        markerColor: 'red',\n        prefix: 'glyphicon'\n    });\n    var marker = L.marker(new L.LatLng(row[0], row[1]), {icon: icon});\n    var popup = L.popup().setContent('&lt;b&gt;Date:&lt;/b&gt; ' + row[2] + '&lt;br&gt;&lt;b&gt;Time:&lt;/b&gt; ' + row[3] + '&lt;br&gt;&lt;b&gt;Injured:&lt;/b&gt; ' + row[4] + '&lt;br&gt;&lt;b&gt;Killed:&lt;/b&gt; ' + row[5] + '&lt;br&gt;&lt;b&gt;Cluster ID:&lt;/b&gt; ' + row[6]);\n    marker.bindPopup(popup);\n    return marker;\n}\n\"\"\"\nfolium.plugins.FastMarkerCluster(data=cluster_map_data.values.tolist(), callback=js_callback).add_to(cluster_map)\n\n# Save and show the map\nIMG_DIR = \"output/hotspots\"\nos.makedirs(IMG_DIR, exist_ok=True)\ncluster_map.save(os.path.join(IMG_DIR, \"serious_clusters_map.html\"))\n\n# Display the map if in a Jupyter notebook\ncluster_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nDangerous Areas for Pedestrians and Cyclists\nDangerous areas for pedestrians and cyclists are clusters of collisions with a high number of pedestrian / cyclist injuries and deaths from vehicle collisions. Clusters were identified by tuning a density-based clustering algorithm.\n\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nimport folium\nimport os\n\n# Load your dataset if not already loaded\n# data = pd.read_csv('path_to_your_file.csv')  # Uncomment and set path if needed\n\n# Define non-motorist collisions based on specific conditions\nnon_motorist = data[\n    (data['number_of_pedestrians_injured'] &gt; 0) |\n    (data['number_of_cyclist_injured'] &gt; 0) |\n    (data['number_of_pedestrians_killed'] &gt; 0) |\n    (data['number_of_cyclist_killed'] &gt; 0)\n].copy()\n\n# Prepare latitude and longitude for clustering\nnon_motorist['latitude'] = pd.to_numeric(non_motorist['latitude'], errors='coerce')\nnon_motorist['longitude'] = pd.to_numeric(non_motorist['longitude'], errors='coerce')\nnon_motorist.dropna(subset=['latitude', 'longitude'], inplace=True)\n\n# Parameters for DBSCAN\neps = 0.0005  # Smaller neighborhood size\nmin_samples = 5  # Minimum samples in a neighborhood to form a cluster\n\n# Perform DBSCAN clustering\ndbscan_model = DBSCAN(eps=eps, min_samples=min_samples, metric=\"euclidean\")\nnon_motorist['cluster'] = dbscan_model.fit_predict(non_motorist[['latitude', 'longitude']])\n\n# Calculate danger scores based on the injuries and fatalities of non-motorists\nFATALITY_MULTIPLE = 10  # Weight assigned to fatalities relative to injuries\nnon_motorist['danger'] = (\n    non_motorist['number_of_pedestrians_injured'] +\n    non_motorist['number_of_cyclist_injured'] +\n    (non_motorist['number_of_pedestrians_killed'] * FATALITY_MULTIPLE) +\n    (non_motorist['number_of_cyclist_killed'] * FATALITY_MULTIPLE)\n)\n\n# Sum danger scores within clusters and identify top clusters\ncluster_groupby = non_motorist[non_motorist['cluster'] &gt; -1].groupby('cluster')['danger'].sum()\ntop_clusters = cluster_groupby.nlargest(200).index.tolist()\n\n# Select data for visualization\ncluster_mask = non_motorist['cluster'].isin(top_clusters)\nfields_to_use = ['latitude', 'longitude', 'crash_date', 'crash_time', 'number_of_pedestrians_injured', 'number_of_cyclist_injured', 'number_of_pedestrians_killed', 'number_of_cyclist_killed', 'cluster']\ncluster_map_data = non_motorist.loc[cluster_mask, fields_to_use]\n\n# Convert datetime fields to string for JavaScript compatibility\ncluster_map_data['crash_date'] = cluster_map_data['crash_date'].astype(str)\ncluster_map_data['crash_time'] = cluster_map_data['crash_time'].astype(str)\n\n# Create a Folium map centered around the average location\nNYC_MAP_CENTER = [non_motorist['latitude'].mean(), non_motorist['longitude'].mean()]\nnm_cluster_map = folium.Map(location=NYC_MAP_CENTER, zoom_start=12, tiles='OpenStreetMap')\n\n# JavaScript callback for custom markers in Folium\njs_callback = \"\"\"\nfunction (row) {\n    var icon = L.AwesomeMarkers.icon({\n        icon: 'info-sign',\n        markerColor: 'red',\n        prefix: 'glyphicon'\n    });\n    var marker = L.marker(new L.LatLng(row[0], row[1]), {icon: icon});\n    var popup = L.popup().setContent('&lt;b&gt;Date:&lt;/b&gt; ' + row[2] + '&lt;br&gt;&lt;b&gt;Time:&lt;/b&gt; ' + row[3] + '&lt;br&gt;&lt;b&gt;Pedestrians Injured:&lt;/b&gt; ' + row[4] + '&lt;br&gt;&lt;b&gt;Cyclists Injured:&lt;/b&gt; ' + row[5] + '&lt;br&gt;&lt;b&gt;Pedestrians Killed:&lt;/b&gt; ' + row[6] + '&lt;br&gt;&lt;b&gt;Cyclists Killed:&lt;/b&gt; ' + row[7] + '&lt;br&gt;&lt;b&gt;Cluster ID:&lt;/b&gt; ' + row[8]);\n    marker.bindPopup(popup);\n    return marker;\n}\n\"\"\"\nfolium.plugins.FastMarkerCluster(data=cluster_map_data.values.tolist(), callback=js_callback).add_to(nm_cluster_map)\n\n# Save the map to an output directory\nIMG_DIR = \"output/hotspots\"\nos.makedirs(IMG_DIR, exist_ok=True)\nnm_cluster_map.save(os.path.join(IMG_DIR, \"clusters_non_motor_map.html\"))\n\n# Display the map if in a Jupyter notebook\nnm_cluster_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Home",
      "Data Wrangling: Analysis",
      "10: Statistical analisys"
    ]
  }
]